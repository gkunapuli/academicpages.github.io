---
title: "Multi-Agent Inverse Reinforcement Learning"
collection: publications
permalink: /publication/10multiagentinverserl
excerpt: ''
date: 2010-06-01
venue: 'Ninth International Conference on Machine Learning and Applications (ICMLA''10), Washington D. C'
paperurl: 'http://gkunapuli.github.io/files/10multiagentinverserl.pdf'
citation: 'S. Natarajan, G. Kunapuli, K. Judah, P. Tadepalli, K. Kersting and J. W. Shavlik. <b> Multi-Agent Inverse Reinforcement Learning. </b>
<i> Ninth International Conference on Machine Learning and Applications </i> (ICMLA''10), Washington D. C., USA, December 12-14, 2010.'
author: 'S. Natarajan, <b>G. Kunapuli</b>, K. Judah, P. Tadepalli, K. Kersting and J. W. Shavlik'
---
Inductive Logic Programming (ILP) provides an effective method of learning logical theories given a set of positive examples, a set of negative examples, a corpus of background knowledge and specification of a search space (e.g., by mode definitions) from which to compose the theories. While specifying positive and negative examples is relatively straightforward, composing effective background knowledge and search space definition requires detailed understanding of many aspects of the ILP process and limits the usability of ILP. This paper introduces a number of techniques to automate the use of ILP for a non-ILP expert. These techniques include automatic generation of background knowledge from user-supplied information in the form of a simple relevance language, utilization of type hierarchies to constrain search, automatic generation of negative examples, and an iterative-deepening style search process.

[[BibTeX]](http://gkunapuli.github.io/files/10multiagentinverserl.bib)

