@inproceedings{DhamiKDPN18,
  author="Nandini Ramanan and Gautam Kunapuli and Tushar Khot and Bahare Fatemi and Seyed Mehran Kazemi and David Poole and Kristian Kersting and Sriraam Natarajan",
  title="Structure Learning for Relational Logistic Regression: An Ensemble Approach",
  booktitle="Proc. International Conference on Principles of Knowledge Representation and Reasoning",
  year=2018,
  abstract="We consider the problem of learning Relational Logistic Regression (RLR). Unlike standard logistic regression, the features of RLRs are first-order formulae with associated weight vectors instead of scalar weights. We turn the problem of learning RLR to learning these vector-weighted formulae and develop a learning algorithm based on the recently successful functional-gradient boosting methods for probabilistic logic models. We derive the functional gradients and show how weights can be learned simultaneously in an efficient manner. Our empirical evaluation on standard and novel data sets demonstrates the superiority of our approach over other methods for learning RLR."
}

---
title: "Structure Learning for Relational Logistic Regression: An Ensemble Approach"
collection: publications
permalink: /publication/18rlrKR
excerpt: ''
date: 2018-10-26
venue: 'International Conference on Principles of Knowledge Representation and Reasoning (KR''18), Tempe, AZ'
paperurl: 'http://gkunapuli.github.io/files/18rlrKR.pdf'
citation: 'N. Ramanan, G. Kunapuli, T. Khot, B. Fatemi, S. M. Kazemi, D. Poole, K. Kersting and Sriraam Natarajan. 
<b> Structure Learning for Relational Logistic Regression: An Ensemble Approach</b>. 
<i> Proc. International Conference on Principles of Knowledge Representation and Reasoning </i> (2018).'
author:  'N. Ramanan, <b>G. Kunapuli</b>, T. Khot, B. Fatemi, S. M. Kazemi, D. Poole, K. Kersting and Sriraam Natarajan'
---
We consider the problem of learning Relational Logistic Regression (RLR). Unlike standard logistic regression, the features of RLRs are first-order formulae with associated weight vectors instead of scalar weights. We turn the problem of learning RLR to learning these vector-weighted formulae and develop a learning algorithm based on the recently successful functional-gradient boosting methods for probabilistic logic models. We derive the functional gradients and show how weights can be learned simultaneously in an efficient manner. Our empirical evaluation on standard and novel data sets demonstrates the superiority of our approach over other methods for learning RLR.

[[BibTeX]](http://gkunapuli.github.io/files/18rlrKR.bib) [[arXiv]](https://arxiv.org/abs/1808.02123)
