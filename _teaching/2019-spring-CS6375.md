---
title: "CS6375: Machine Learning"
collection: teaching
type: "3 Credit Course"
permalink: /teaching/2019-spring-CS6375
venue: "JSOM 2.802"
date: 2019-01-01
location: "UT Dallas"
semester: "Spring 2019"
---

Spring 2019

![#f03c15](https://placehold.it/15/f03c15/000000?text=+) <span style="color: red">**The final exam will be held at JSON 2.804.**</span> 

Course Overview
======
**Class Hours**: Mo/We 1:00â€“2:15pm <br> 
**Class Room**: JSOM 2.802 <br>

**Instructor**: Gautam Kunapuli <br>
**Office**: ECSS 2.717 <br>
**Email**:  Gautam-dot-Kunapuli-@-utdallas-dot-edu <br>
**Office Hours**: Wednesdays, 2:30pm-4:30pm; and by appointment <br>

**Teaching Assistant**: TBA <br>
**Email**: TBA <br>
**Office Hours**: TBA <br>


Course Description
-----
The main aim of the course is to provide an introduction and **hands-on understanding** of a broad variety of **machine-learning algorithms** on **real applications**. In addition to delving into the underlying mathematical and algorithmic details for many learning methods, we will also explore the practical aspects of applying machine learning to real-world data through **programming assignments**.

Pre-requisities 
----
The mandatory pre-requisite is **CS5343: Algorithm Analysis and Data Structures**. 

In addition, many concepts in this class require a comfortable grasp of basic **probability theory**, **linear algebra**, **multivariate calculus** and **optimization**. Garret Thomas' _Mathematics for Machine Learning_ is a superb **review of essential mathematical background**: you can find it [here](https://gwthomas.github.io/docs/math4ml.pdf).

Python Resources
----
The programming assignments will require coding in **Python**. The following books may be useful as a **quick introduction to Python**:
* [_A Whirlwind Tour of Python_](https://www.oreilly.com/programming/free/files/a-whirlwind-tour-of-python.pdf) (and its companion [repository of Jupyter Notebooks](https://github.com/jakevdp/WhirlwindTourOfPython)) by Jake VanderPlas is "...a fast-paced introduction to essential components of the Python language for researchers and developers who are already familiar with programming in another language" [author];
* [_Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas (and its companion [repository of Jupyter Notebooks](https://github.com/jakevdp/PythonDataScienceHandbook)) "introduces the core libraries essential for working with data in Python: particularly IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and related packages" [author].

The following books are also useful references if you want to learn Python from scratch:
* [_Think Python: How to Think Like a Computer Scientist_](http://greenteapress.com/thinkpython/html/index.html) by Allen B. Downey;
* [_Automate the Boring Stuff with Python_](https://automatetheboringstuff.com/) by Al Sweigart.


Textbooks and Course Materials
----
There is **no required textbook** for this class. However, the following textbooks are useful references for various topics we will cover in this course:
* _Pattern Recognition and Machine Learning_ by Christopher M. Bishop; this is a standard textbook and reference for introductory
machine learning and covers a large part of our syllabus;
* _Machine Learning: a Probabilistic Perspective_ by Kevin Murphy; another excellent book and reference, especially for probabilistic
graphical models.

The following books are **available online, free for personal use**. Supplemental reading material will be assigned from these sources
as often as possible.
* _The Elements of Statistical Learning: Data Mining, Inference, and Prediction_ by Trevor Hastie, Robert Tibshirani and Jerome Friedman (available [online](https://web.stanford.edu/~hastie/ElemStatLearn/))
* _Bayesian Reasoning and Machine Learning_ by David Barber (available [online](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online))
* _Understanding Machine Learning: From Theory to Algorithms_ by Shai Shalev-Shwartz and Shai Ben-David (available [online](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning)) introduces machine learning from a theoretical perspective;
* _Deep Learning_ by Ian Goodfellow, Yoshua Bengio and Aaron Courville (available [online](https://www.deeplearningbook.org/)) is an excellent introductory textbook for a wide-variety of deep learning methods and applications;
* _Reinforcement Learning: An Introduction_ by Richard S. Sutton and Andrew G. Barto (available [online](http://incompleteideas.net/book/the-book-2nd.html)) is the _de facto_ textbook and reference for reinforcement learning;


Syllabus and Schedule
======

| Week | Date | Topic | Readings | Notes |
|---|---|---|---|---|
| 1 | Jan 14 (mo) | [Introduction](https://gkunapuli.github.io/files/cs6375/01-IntroductionToML.pdf) & [Linear Regression](https://gkunapuli.github.io/files/cs6375/02-LinearRegression.pdf) | Bishop, Ch. 1|  |
|   | Jan 16 (we) | [Linear Regression](https://gkunapuli.github.io/files/cs6375/02-LinearRegression.pdf) (continued) | [Andrew Ng's Lecture Notes, Part I](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes1.pdf); <br> Shalev-Shwartz & Ben-David, Ch. 9.2; <br> [Kilian Weinberger's Lecture Notes  (probabilistic view)](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote08.html)  |  |
| 2 | Jan 21 (mo) |  **Martin Luther King Day** <br> _No class_  |    |    |
|   | Jan 23 (we) | [Perceptron](https://gkunapuli.github.io/files/cs6375/03-Perceptron.pdf) |  Shalev-Shwartz & Ben-David, Ch. 9.1; <br> [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote03.html)   |    |
| 3 | Jan 28 (mo) |  Perceptron (continued)  |  [Computational complexity of <br> GD vs. Stochastic GD](https://stanford.edu/~rezab/classes/cme323/S15/notes/lec11.pdf)  |  **HW1 Out**   |
|   | Jan 30 (we) | [Support Vector Machines](https://gkunapuli.github.io/files/cs6375/06-SupportVectorMachines.pdf)  | [Andrew Ng's Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes3.pdf);<br> Bishop, Ch. 7; <br> Barber, Ch. 17.5; <br> Shalev-Shwartz & Ben-David, Ch. 15   |   |
| 4 | Feb 4 (mo) |   Support Vector Machines (continued)  |    |    |
|   | Feb 6 (we) |   [Decision Trees](https://gkunapuli.github.io/files/cs6375/04-DecisionTrees.pdf)  | [Mitchell, Ch. 3](http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf); <br> [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote17.html) |  |
| 5 | Feb 11 (mo) |  Decision Trees (continued) |    |  **HW 1 Due**  <br> **HW 2 Out**|
|   | Feb 13 (we) | [Nearest Neighbor Methods](https://gkunapuli.github.io/files/cs6375/05-NearestNeighborMethods.pdf) | Bishop, Ch. 14.4; <br> [Daum&#233; III, Ch. 3](http://ciml.info/dl/v0_99/ciml-v0_99-ch03.pdf) |   |
| 6 | Feb 18 (mo) |   [Good Machine Learning Practices](https://gkunapuli.github.io/files/cs6375/08-MachineLearningPractice.pdf) <br> _pre-processing, model selection, <br> cross validation, missing data, evaluation_ <br>| [Kotsiantis et al., 2006](https://pdfs.semanticscholar.org/c640/1e515a58fc36c37fc97e3b0cb18ce4682743.pdf) |   |
|   | Feb 20 (we) |   Good Machine Learning Practices (continued)|  |   |
| 7 | Feb 25 (mo) |  [Naive Bayes](https://gkunapuli.github.io/files/cs6375/09-NaiveBayes.pdf) | [Jerry Zhu's Lecture Notes](http://pages.cs.wisc.edu/~jerryzhu/cs769/nb.pdf); <br> [Mitchell 2nd ed. Ch. 3.1-3.2](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf); <br> [Daum&#233; III, Ch. 9](http://ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf) |   |
|   | Feb 27 (we) |   [Logistic Regression](https://gkunapuli.github.io/files/cs6375/10-LogisticRegression.pdf)   |  [Mitchell 2nd ed. Ch. 3.3-3.5](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf); <br> Bishop, 8.4.1, 9.2, 9.3, 9.4; <br> [Andrew Ng's Lecture Notes, Pt II](http://cs229.stanford.edu/notes/cs229-notes1.pdf); <br> [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote06.html)  |  **HW 2 Due** <br> **HW3 Out**  |
| 8 | Mar 4 (mo) |  ![#3ca015](https://placehold.it/15/3ca015/000000?text=+) [Mid-Term Exam Prep](https://gkunapuli.github.io/files/cs6375/MidTerm-Review.pdf)  |    |    |
|   | Mar 6 (we) |   ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) **Mid-Term Exam**  |    | <span style="color: red"> **JSOM 2.115<br> 1-2:15pm** </span>   |
| 9 | Mar 11 (mo) |    [Ensemble Methods: Bagging](https://gkunapuli.github.io/files/cs6375/11-Bagging.pdf)  | Bishop, Ch. 14; <br> [Hastie et al., Ch. 7.1-7.6, 8.7](https://web.stanford.edu/~hastie/ElemStatLearn/); <br> [Visualization of the Bias-Variance Tradeoff](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)|   |
|   | Mar 13 (we) | [Ensemble Methods: Boosting](https://gkunapuli.github.io/files/cs6375/12-Boosting.pdf)  | [Hastie et al., Ch. 15](https://web.stanford.edu/~hastie/ElemStatLearn/); <br> [Freund and Schapire, 1999](http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) |   |
|10 | Mar 18 (mo) | **Spring Break** <br> _No class_   |    |    |
|   | Mar 20 (we) | **Spring Break** <br> _No class_  |    |    |
|11 | Mar 25 (mo) |  Ensemble Methods: Boosting (continued)  |    |    |
|   | Mar 27 (we) |  [Ensemble Methods: Gradient Boosting](https://gkunapuli.github.io/files/cs6375/13-GradientBoosting.pdf)  | [Friedman, 99](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf); <br> [Mason et al., 99](http://maths.dur.ac.uk/~dma6kp/pdf/face_recognition/Boosting/Mason99AnyboostLong.pdf); <br> [Visualizing Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html); <br> [Tong He's Presentation on XGBoost](https://www.youtube.com/watch?v=ufHo8vbk6g4)    |   |
|12 | Apr 1 (mo) |     [Principal Components Analysis](https://gkunapuli.github.io/files/cs6375/14-PrincipalComponentAnalysis.pdf) | [Andrew Ng's Lecture Notes, Pt II](http://cs229.stanford.edu/notes/cs229-notes10.pdf)    |  **HW 3 Due** <br> **HW4 Out**   |
|   | Apr 3 (we) |  [Clustering](https://gkunapuli.github.io/files/cs6375/15-Clustering.pdf)   | [Tan et al., Ch. 8](https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf)  |    |
|13 | Apr 8 (mo) |  Clustering (continued)  |    |    |
|   | Apr 10 (we) | [Neural Networks](https://gkunapuli.github.io/files/cs6375/16-NeuralNetworks.pdf)  | Goodfellow et al., Ch. 6  |    |
|14 | Apr 15 (mo) | Neural Networks (continued)   |    |  **HW 4 Due** <br> **HW5 Out**  |
|   | Apr 17 (we) |  [Convolutional Neural Networks](https://gkunapuli.github.io/files/cs6375/17-CNNs.pdf) | Goodfellow et al., Ch. 9  |      |
|15 | Apr 22 (mo) |    [Reinforcement Learning](https://gkunapuli.github.io/files/cs6375/20-ReinforcementLearning.pdf) | Sutton and Barto, Ch. 1, 3; <br> [Andrew Ng's Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes12.pdf)  |      |
|   | Apr 24 (we) |  Reinforcement Learning (continued)  |    |    |
|16 | Apr 29 (mo) |  ![#3ca015](https://placehold.it/15/3ca015/000000?text=+)[Final Exam Prep](https://gkunapuli.github.io/files/cs6375/Final-Review.pdf)   |    | **HW 5 Due**   |
|   | May 1 (we)  |  ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) **In-class Final Exam**   |    |  <span style="color: red"> **JSOM 2.804<br> 1-2:15pm** </span>  |

The topic schedule is subject to change at the instructor's discretion. Please check this page regularly for lecture slides, additional references and reading materials.

Grading
======
* 50%, Homework Problem Sets/Programming Assignments (5, each 10%)
* 20%, Mid-term Exam
* 30%, Final Exam


Course Policies
=====

Attendance Policy
----
Classroom attendance for all lectures is mandatory. Prolonged absence from the lectures may lead to substantial grade penalties:
* two consecutive absences, no penalty;
* 3 consecutive absences: 1 letter grade drop;
* 4 consecutive absences, F grade.

Absence due to emergency or extenuating circumstances can be excused, but [proof may be required](http://cs.utdallas.edu/education/undergraduate/attendance-policy/).

Homework Policy
----
Homework assignments are **due at the start of class on the due date** without exceptions, unless permission was obtained from the instructor **in advance**. Homework and assignment deadlines will  not be extended except under extreme university-wide circumstances such as weather emergencies.

All homeworks, programming projects, take-home exams (if any) **are to be written up and completed individually**. You **may discuss, collaborate, brainstorm and strategize** ideas, concepts and problems with other students. However, all written solutions and coded programs **must be your own**. Copying another student's work or allowing other students to copy your work is academically dishonest.

Academic Integrity
----
All students are responsible for adhering to UT Dallas Community Standards and Conduct, particularly regarding [Academic Integrity](https://www.utdallas.edu/conduct/integrity/) and [Academic Dishonesty](https://www.utdallas.edu/conduct/dishonesty/).  Any academic dishonesty, including, but not restricted to [plagiarism](https://www.utdallas.edu/conduct/dishonesty/#plagiarism) (including from internet sources), [collusion](https://www.utdallas.edu/conduct/dishonesty/#collusion), [cheating](https://www.utdallas.edu/conduct/dishonesty/#cheating), [fabrication](https://www.utdallas.edu/conduct/dishonesty/#fabrication), will result in a zero score on the assignment/project/exam and possible disciplinary action.

Students with Disabilities
----
UT Dallas is committed to equal access in all endeavors for students with disabilities. The [Office of Student Accessability (OSA)](https://www.utdallas.edu/studentaccess/) provides academic accommodations for eligible students with a documented disability. Accommodations for each student are determined by OSA on an individual basis, with input from qualified professionals. Accommodations are intended to level the playing field for students with disabilities, while maintaining the academic integrity and standards set by the University. If you think you qualify for an academic accommodation, please visit OSA to determine eligibility. 

If you have already received academic accommodation, please contact me by e-mail to schedule an appointment **before classes start**, if possible.








