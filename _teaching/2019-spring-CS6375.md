---
title: "CS6375: Machine Learning"
collection: teaching
type: "3 Credit Course"
permalink: /teaching/2019-spring-CS6375
venue: "JSOM 2.802"
date: 2019-01-01
location: "UT Dallas"
semester: "Spring 2019"
---

Spring 2019

Course Overview
======
**Class Hours**: Mo/We 1:00â€“2:15pm <br> **Class Room**: JSOM 2.802 <br>

**Instructor**: Gautam Kunapuli <br>
**Office**: ECSS 2.717 <br>
**Email**:  gautam-dot-kunapuli-@-utdallas.edu <br>
**Office Hours**: Wednesdays, 2:30pm-4:30pm; and by appointment <br>

**Teaching Assistant**: TBA <br>
**Email**: TBA <br>
**Office Hours**:  TBA <br>

Course Description
-----
The main aim of the course is to provide an introduction and **hands-on understanding** of a broad variety of **machine-learning algorithms** on **real applications**. In addition to delving into the underlying mathematical and algorithmic details for many learning methods, we will also explore the practical aspects of applying machine learning to real-world data through **programming assignments**.

Pre-requisities 
----
The mandatory pre-requisite is **CS5343: Algorithm Analysis and Data Structures**. 

In addition, many concepts in this class require a comfortable grasp of basic **probability theory**, **linear algebra**, **multivariate calculus** and **optimization**. Garret Thomas' _Mathematics for Machine Learning_ is a superb **review of essential mathematical background**: you can find it [here](https://gwthomas.github.io/docs/math4ml.pdf).

Python Resources
----
The programming assignments will require coding in **Python**. The following books may be useful as a **quick introduction to Python**:
* [_A Whirlwind Tour of Python_](https://www.oreilly.com/programming/free/files/a-whirlwind-tour-of-python.pdf) (and its companion [repository of Jupyter Notebooks](https://github.com/jakevdp/WhirlwindTourOfPython)) by Jake VanderPlas is "...a fast-paced introduction to essential components of the Python language for researchers and developers who are already familiar with programming in another language" [author];
* [_Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas (and its companion [repository of Jupyter Notebooks](https://github.com/jakevdp/PythonDataScienceHandbook)) "introduces the core libraries essential for working with data in Python: particularly IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and related packages" [author].

The following books are also useful references if you want to learn Python from scratch:
* [_Think Python: How to Think Like a Computer Scientist_](http://greenteapress.com/thinkpython/html/index.html) by Allen B. Downey;
* [_Automate the Boring Stuff with Python_](https://automatetheboringstuff.com/) by Al Sweigart.


Textbooks and Course Materials
----
There is **no required textbook** for this class. However, the following textbooks are useful references for various topics we will cover in this course:
* _Pattern Recognition and Machine Learning_ by Christopher M. Bishop; this is a standard textbook and reference for introductory
machine learning and covers a large part of our syllabus;
* _Machine Learning: a Probabilistic Perspective_ by Kevin Murphy; another excellent book and reference, especially for probabilistic
graphical models.

The following books are **available online, free for personal use**. Supplemental reading material will be assigned from these sources
as often as possible.
* _The Elements of Statistical Learning: Data Mining, Inference, and Prediction_ by Trevor Hastie, Robert Tibshirani and Jerome Friedman (available [online](https://web.stanford.edu/~hastie/ElemStatLearn/))
* _Introduction to Data Mining_ by 	Pang-Ning Tan, Michael Steinbach, Anuj Karpatne and Vipin Kumar (available [online](https://www-users.cs.umn.edu/~kumar001/dmbook/index.php))
* _Bayesian Reasoning and Machine Learning_ by David Barber (available [online](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online))
* _Understanding Machine Learning: From Theory to Algorithms_ by Shai Shalev-Shwartz and Shai Ben-David (available [online](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning)) introduces machine learning from a theoretical perspective;
* _Deep Learning_ by Ian Goodfellow, Yoshua Bengio and Aaron Courville (available [online](https://www.deeplearningbook.org/)) is an excellent introductory textbook for a wide-variety of deep learning methods and applications;
* _Reinforcement Learning: An Introduction_ by Richard S. Sutton and Andrew G. Barto (available [online](http://incompleteideas.net/book/the-book-2nd.html)) is the _de facto_ textbook and reference for reinforcement learning;


Syllabus and Schedule
======

| Week | Date | Topic | Readings | Notes |
|---|---|---|---|---|
| 1 | Jan 14 (mo) | [Introduction](https://gkunapuli.github.io/files/cs6375/01-IntroductionToML.pdf) & [Linear Regression](https://gkunapuli.github.io/files/cs6375/02-LinearRegression.pdf) | Bishop, Ch. 1|  |
|   | Jan 16 (we) | [Linear Regression](https://gkunapuli.github.io/files/cs6375/02-LinearRegression.pdf) (continued) | [Andrew Ng's Lecture Notes, Part I](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes1.pdf); <br> Shalev-Shwartz & Ben-David, Ch. 9.2; <br> [Kilian Weinberger's Lecture Notes  (probabilistic view)](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote08.html)  |  |
| 2 | Jan 21 (mo) |  **Martin Luther King Day** <br> _No class_  |    |    |
|   | Jan 23 (we) | [Perceptron](https://gkunapuli.github.io/files/cs6375/03-Perceptron.pdf) |  Shalev-Shwartz & Ben-David, Ch. 9.1; <br> [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote03.html)   |    |
| 3 | Jan 28 (mo) |  Perceptron (continued)  |  [Computational complexity of <br> GD vs. Stochastic GD](https://stanford.edu/~rezab/classes/cme323/S15/notes/lec11.pdf)  |  **HW1 Out**   |
|   | Jan 30 (we) | [Support Vector Machines](https://gkunapuli.github.io/files/cs6375/06-SupportVectorMachines.pdf)  | [Andrew Ng's Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes3.pdf);<br> Bishop, Ch. 7; <br> Barber, Ch. 17.5; <br> Shalev-Shwartz & Ben-David, Ch. 15   |   |
| 4 | Feb 4 (mo) |   Support Vector Machines (continued)  |    |    |
|   | Feb 6 (we) |    |    |    |
| 5 | Feb 11 (mo) |    |    |  **HW 1 Due**  <br> **HW 2 Out**|
|   | Feb 13 (we) |    |    |     |
| 6 | Feb 18 (mo) |    |    |    |
|   | Feb 20 (we) |    |    |    |
| 7 | Feb 25 (mo) |    |    |    |
|   | Feb 27 (we) |    |    |  **HW 2 Due** <br> **HW3 Out**  |
| 8 | Mar 4 (mo) |    |    |    |
|   | Mar 6 (we) |    |    |    |
| 9 | Mar 11 (mo) | Mid-Term Exam Prep   |    |    |
|   | Mar 13 (we) | ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) **In-class Mid-Term Exam**   |    |  **HW 3 Due**  |
|10 | Mar 18 (mo) | **Spring Break** <br> _No class_   |    |    |
|   | Mar 20 (we) | **Spring Break** <br> _No class_  |    |    |
|11 | Mar 25 (mo) |    |    |    |
|   | Mar 27 (we) |    |    |  **HW4 Out**  |
|12 | Apr 1 (mo) |    |    |    |
|   | Apr 3 (we) |    |    |    |
|13 | Apr 8 (mo) |    |    |    |
|   | Apr 10 (we) |    |    |  **HW 4 Due** <br> **HW5 Out**  |
|14 | Apr 15 (mo) |    |    |    |
|   | Apr 17 (we) |    |    |    |
|15 | Apr 22 (mo) |    |    |    |
|   | Apr 24 (we) |    |    |  **HW 5 Due**  |
|16 | Apr 29 (mo) |  Final Exam Prep  |    |    |
|   | May 1 (we)  |  ![#f03c15](https://placehold.it/15/f03c15/000000?text=+) **In-class Final Exam**   |    |    |



The topic schedule is subject to change at the instructor's discretion. Please check this page regularly for lecture slides, additional references and reading materials.

Tentative List of topics: Linear regression, Perceptron, Decision Trees, Support Vector Machines, Nearest Neighbor Methods, Naive Bayes, Logistic Regression, Ensemble Methods (bagging, boosting, gradient boosting), Dimensionality Reduction, Clustering, Neural Networks & Deep Learning, Hidden Markov Models and Reinforcement Learning.
In addition, we will also cover the basics of machine learning theory (PAC bounds, VC dimension) and machine learning practice (representation, model selection and evaluation).

Grading
======
* 50%, Homework Problem Sets/Programming Assignments (5, each 10%)
* 20%, Mid-term Exam
* 30%, Final Exam


Course Policies
=====

Attendance Policy
----
Classroom attendance for all lectures is mandatory. Prolonged absence from the lectures may lead to substantial grade penalties:
* two consecutive absences, no penalty;
* 3 consecutive absences: 1 letter grade drop;
* 4 consecutive absences, F grade.

Absence due to emergency or extenuating circumstances can be excused, but [proof may be required](http://cs.utdallas.edu/education/undergraduate/attendance-policy/).

Homework Policy
----
Homework assignments are **due at the start of class on the due date** without exceptions, unless permission was obtained from the instructor **in advance**. Homework and assignment deadlines will  not be extended except under extreme university-wide circumstances such as weather emergencies.

All homeworks, programming projects, take-home exams (if any) **are to be written up and completed individually**. You **may discuss, collaborate, brainstorm and strategize** ideas, concepts and problems with other students. However, all written solutions and coded programs **must be your own**. Copying another student's work or allowing other students to copy your work is academically dishonest.

Academic Integrity
----
All students are responsible for adhering to UT Dallas Community Standards and Conduct, particularly regarding [Academic Integrity](https://www.utdallas.edu/conduct/integrity/) and [Academic Dishonesty](https://www.utdallas.edu/conduct/dishonesty/).  Any academic dishonesty, including, but not restricted to [plagiarism](https://www.utdallas.edu/conduct/dishonesty/#plagiarism) (including from internet sources), [collusion](https://www.utdallas.edu/conduct/dishonesty/#collusion), [cheating](https://www.utdallas.edu/conduct/dishonesty/#cheating), [fabrication](https://www.utdallas.edu/conduct/dishonesty/#fabrication), will result in a zero score on the assignment/project/exam and possible disciplinary action.

Students with Disabilities
----
UT Dallas is committed to equal access in all endeavors for students with disabilities. The [Office of Student Accessability (OSA)](https://www.utdallas.edu/studentaccess/) provides academic accommodations for eligible students with a documented disability. Accommodations for each student are determined by OSA on an individual basis, with input from qualified professionals. Accommodations are intended to level the playing field for students with disabilities, while maintaining the academic integrity and standards set by the University. If you think you qualify for an academic accommodation, please visit OSA to determine eligibility. 

If you have already received academic accommodation, please contact me by e-mail to schedule an appointment **before classes start**, if possible.








