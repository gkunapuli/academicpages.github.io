---
title: "CS6375: Machine Learning"
collection: teaching
type: "3 Credit Course"
permalink: /teaching/2018-fall-CS6375
venue: "JO 3.516"
date: 2018-01-01
location: "UT Dallas"
---
Fall 2018



Course Overview
======
**Class Hours**: Tu/Th 4:00â€“5:15pm <br> **Class Room**: JO 3.516 <br>

Instructor: Gautam Kunapuli <br>
Office: ECSS 2.717 <br>
Email:  gautam-dot-kunapuli-@-utdallas.edu <br>
Office Hours: Wed 12pm-1pm, and by appointment. <br>

Teaching Assistant: TBA <br>
Office: TBA <br> 
Email: TBA <br>
Office Hours:  TBA <br>

**Course Description:** The main aim of the course is to provide an introduction and **hands-on experience and understanding**
of a broad variety of **machine-learning algorithms** on **real applications**. In addition to delving into the underlying mathematical and/or algorithmic details for many learning algorithms, we will also explore the practical aspects of applying machine learning to real-world data through **programming assignments**.

**Pre-requisities:** The mandatory pre-requisite is **CS5343: Algorithm Analysis and Data Structures**. 

In addition, many concepts in this class require a comfortable grasp of basic **probability theory**, **linear algebra**, **multivariate calculus** and **optimization**. Garret Thomas' _Mathematics for Machine Learning_ is a superb review of essential mathematical background, both to review and as a quick reference; you can read it [here](https://gwthomas.github.io/docs/math4ml.pdf).

The programming assignments will require coding in **Python**.

**Textbooks and Course Materials:**
There is no required textbook for this class. 

The following textbooks are useful references:
* _Pattern Recognition and Machine Learning_ by Christopher M. Bishop is a standard machine learning textbook and covers much of the syllabus for this class;
* _Deep Learning_ by Ian Goodfellow, Yoshua Bengio and Aaron Courville (available [online](https://www.deeplearningbook.org/)) is an excellent introductory textbook to a wide-variety of deep learning methods and applications;
* _Reinforcement Learning: An Introduction_ by Richard S. Sutton and Andrew G. Barto (available [online](http://incompleteideas.net/book/the-book-2nd.html)) is the _de facto_ standard textbook and reference for reinforcement learning;
* _Bayesian Reasoning and Machine Learning_ by David Barber (available [online](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.phpBa?n=Brml.Online))


**Grading policy:** TBA

**Attendance policy:**
Two consecutive absences, no penalty; <b> 3 consecutive absences, 1 letter grade drop</b>; <b> <u> 4 consecutive absences, F grade</u></b>. Absences due to medical reasons, death in family, or other extreme circumstances can be excused but [proof may be required](http://cs.utdallas.edu/education/undergraduate/attendance-policy/).

**Homework policy:** Homework assignments are **due by the start of class on the due date** unless otherwise specified, without exceptions.

Syllabus and Schedule
======
Slides and other course materials will be posted here.

<!--table border="1" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td width="117" valign="top"><p align="center"><em><strong>Week</strong></em></p></td>
    <td width="321" valign="top"><p align="center"><strong><em>Topic </em></strong></p></td>
    <td width="321" valign="top"><div align="center"><em><strong>Reading</strong></em></div></td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 1</p></td>
    <td width="321" valign="top">Introduction &amp; Features</td>
    <td width="321" valign="top"><a href="/natarasr/Courses/AML/Readings/FeatureSelection.pdf">Feature Selection</a></td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 2</p></td>
    <td width="321" valign="top"><p>Evaluation Methodology &amp; Decision Trees (HW1 Out)</p></td>
    <td width="321" valign="top">Chapter 3 of Mitchell Book </td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 3</p></td>
    <td width="321" valign="top"><p>Support Vector Machines(HW1 due &amp; PA1 Out)</p></td>
    <td width="321" valign="top"><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">Andrew Ng's class notes</a></td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 4</p></td>
    <td width="321" valign="top"><p>Decision Trees and SVM wrap up</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 5</p></td>
    <td width="321" valign="top"><p>Logistic Regression (PA1 due &amp; HW2 Out) &amp; Naive Bayes</p></td>
    <td width="321" valign="top"><a href="http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Chapter 3 of Mitchell Book </a></td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 6</p></td>
    <td width="321" valign="top"><p>Nearest Neighbors &amp; Bias variance Analysis (PA 2 out)</p></td>
    <td width="321" valign="top">See onestart for reading materials</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 7</p></td>
    <td width="321" valign="top"><p>Ensemble Methods (HW3 due)</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 8</p></td>
    <td width="321" valign="top"><p><strong>Mid-term 1 </strong></p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>&nbsp; 9</p></td>
    <td width="321" valign="top"><p>Ensemble Methods  (PA2 Due)</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>10</p></td>
    <td width="321" valign="top"><p>Talk by Dev and Introduction to RL (HW4 due)</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>11</p></td>
    <td width="321" valign="top"><p>RL Continued</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>12</p></td>
    <td width="321" valign="top"><p>Unsupervised Learning </p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>13</p></td>
    <td width="321" valign="top"><p>Unsupervised Learning WrapUp(PA 3 due)</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p> 11/25</p></td>
    <td width="321" valign="top"><p>THANKSGIVING   BREAK </p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>14</p></td>
    <td width="321" valign="top"><p>Project Presentations (HW5 due)</p></td>
    <td width="321" valign="top">&nbsp;</td>
  </tr>
  <tr>
    <td width="117" valign="top"><p>15</p></td>
    <td width="321" valign="top">Projects due, Final Exam and Wrap Up</td>
    <td width="321" valign="top"><p>&nbsp;</p></td>
  </tr>
</tbody></table-->



Homeworks, Exams and Other Materials
======


Academic Integrity
======
All students are responsible for adherering to UT Dallas Community Standards and Conduct, particularly regarding [Academic Integrity](https://www.utdallas.edu/conduct/integrity/) and [Academic Dishonesty](https://www.utdallas.edu/conduct/dishonesty/). All homeworks, programming projects, take-home exams (if any) <b> are to be completed individually</b>. You may discuss materials, ideas, concepts and strategies with other students, but <b>all written work must be your own</b>. Any academic dishonesty, including, but not restricted to [plagiarism](https://www.utdallas.edu/conduct/dishonesty/#plagiarism) (including from internet sources), [collusion](https://www.utdallas.edu/conduct/dishonesty/#collusion), [cheating](https://www.utdallas.edu/conduct/dishonesty/#cheating), [fabrication](https://www.utdallas.edu/conduct/dishonesty/#fabrication), will result in a zero score on the assignment/project/exam and possible disciplinary action.


