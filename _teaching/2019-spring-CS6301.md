---
title: "CS6301: Machine Learning for Engineers and Scientists"
collection: teaching
type: "3 Credit Course"
permalink: /teaching/2019-spring-CS6301
venue: "ECSS 2.311"
date: 2019-01-01
location: "UT Dallas"
semester: "Spring 2019"
---

Spring 2019

Course Overview
======
**Class Hours**: Mo/We 10:00â€“11:15am <br> 
**Class Room**: ECSS 2.311 <br>

**Instructor**: Gautam Kunapuli <br>
**Office**: ECSS 2.717 <br>
**Email**:  Gautam-dot-Kunapuli-@-utdallas-dot-edu <br>
**Office Hours**: Wednesdays, 2:30pm-4:30pm; and by appointment <br>

**Teaching Assistant**: Siwen Yan <br>
**Email**: Siwen-dot-Yan-@-utdallas-dot-edu <br>
**Office Hours**: Tuesdays and Fridays, 1:00pm-2:30pm at ECSS 2.104A1 <br>

Course Description
-----
The main aim of the course is to provide Engineers and Scientists with a **hands-on understanding** of a broad variety of **machine-learning algorithms**. The course introduces several key as well as emerging/state-of-the-art machine learning alogrithms with a view toward practical usage and applications. The course aims to explore applications of machine learning in engineering, bioinformatics, economics, computer vision and many others as well as good machine learning practices in data pre-processing, modeling and evaluation.

Python Resources
----
The programming assignments will require coding in **Python** and will primarily use [scikit-learn](http://scikit-learn.org/stable/) to explore various machine-learning algorithms. The following books may be useful as a **quick introduction to Python**:
* [_A Whirlwind Tour of Python_](https://www.oreilly.com/programming/free/files/a-whirlwind-tour-of-python.pdf) (and its companion [repository of Jupyter Notebooks](https://github.com/jakevdp/WhirlwindTourOfPython)) by Jake VanderPlas is "...a fast-paced introduction to essential components of the Python language for researchers and developers who are already familiar with programming in another language" [author];
* [_Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas (and its companion [repository of Jupyter Notebooks](https://github.com/jakevdp/PythonDataScienceHandbook)) "introduces the core libraries essential for working with data in Python: particularly IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and related packages" [author].

The following books are also useful references if you want to learn Python from scratch:
* [_Think Python: How to Think Like a Computer Scientist_](http://greenteapress.com/thinkpython/html/index.html) by Allen B. Downey;
* [_Automate the Boring Stuff with Python_](https://automatetheboringstuff.com/) by Al Sweigart.



Textbooks and Course Materials
----
There is **no required textbook** for this class. However, the following books are useful references for various topics we will cover in this course:
* _Pattern Recognition and Machine Learning_ by Christopher M. Bishop; this is a standard textbook and reference for introductory machine learning;
* _Machine Learning: a Probabilistic Perspective_ by Kevin Murphy; another excellent book and reference, especially for probabilistic graphical models.

The following books are **available online, free for personal use**. Supplemental reading material will be
assigned from these sources as often as possible.
* _The Elements of Statistical Learning: Data Mining, Inference, and Prediction_ by Trevor Hastie, Robert Tibshirani and Jerome Friedman (available [online](https://web.stanford.edu/~hastie/ElemStatLearn/))
* _Deep Learning_ by Ian Goodfellow, Yoshua Bengio and Aaron Courville (available
[online](https://www.deeplearningbook.org/)) is an excellent introductory textbook for a wide-variety of
deep learning methods and applications.

For **even more online resources**, see [_Awesome Machine Learning_](https://github.com/josephmisiti/awesome-machine-learning), Joseph Misiti's curated list of Machine Learning [books](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md), frameworks, libraries and software. 

Syllabus and Schedule
======

| Week | Date | Topic | Readings | Notes |
|---|---|---|---|---|
| 1 | Jan 14 (mo) | [Introduction](https://gkunapuli.github.io/files/cs6375/01-IntroductionToML.pdf) & [Linear Regression](https://gkunapuli.github.io/files/cs6375/02-LinearRegression.pdf) | Bishop, Ch. 1|  |
|   | Jan 16 (we) | [Linear Regression](https://gkunapuli.github.io/files/cs6375/02-LinearRegression.pdf) (continued) | [Andrew Ng's Lecture Notes, Part I](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes1.pdf); <br> Shalev-Shwartz & Ben-David, Ch. 9.2; <br> [Kilian Weinberger's Lecture Notes  (probabilistic view)](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote08.html)  |  |
| 2 | Jan 21 (mo) |  **Martin Luther King Day** <br> _No class_ |    |    |
|   | Jan 23 (we) |  [Perceptron](https://gkunapuli.github.io/files/cs6375/03-Perceptron.pdf)  |  [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote03.html)   |    |
| 3 | Jan 28 (mo) |  Perceptron (continued)   | [Computational complexity of <br> GD vs. Stochastic GD](https://stanford.edu/~rezab/classes/cme323/S15/notes/lec11.pdf) |  **HW1 Out**   |
|   | Jan 30 (we) |  [Support Vector Machines](https://gkunapuli.github.io/files/cs6375/06-SupportVectorMachines.pdf) |  [Andrew Ng's Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes3.pdf);<br> Bishop, Ch. 7   |   |
| 4 | Feb 4 (mo) |  Support Vector Machines (continued)   |    |    |
|   | Feb 6 (we) |  [Decision Trees](https://gkunapuli.github.io/files/cs6375/04-DecisionTrees.pdf)  | [Mitchell, Ch. 3](http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf); <br> [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote17.html) |  |
| 5 | Feb 11 (mo) |  Decision Trees (continued)  |    |  **HW1 Due** <br> **HW2 Out**  |
|   | Feb 13 (we) |  [Nearest Neighbor Methods](https://gkunapuli.github.io/files/cs6375/05-NearestNeighborMethods.pdf)  |   [Daum&#233; III, Ch. 3](http://ciml.info/dl/v0_99/ciml-v0_99-ch03.pdf)  |    |
| 6 | Feb 18 (mo) |  [Good Machine Learning Practices](https://gkunapuli.github.io/files/cs6375/08-MachineLearningPractice.pdf) <br> _pre-processing, model selection, <br> cross validation, missing data, evaluation_ <br>| [Kotsiantis et al., 2006](https://pdfs.semanticscholar.org/c640/1e515a58fc36c37fc97e3b0cb18ce4682743.pdf) |   |
|   | Feb 20 (we) |  Good Machine Learning Practices (continued) |   |   |
| 7 | Feb 25 (mo) |  [Naive Bayes](https://gkunapuli.github.io/files/cs6375/09-NaiveBayes.pdf)  | [Jerry Zhu's Lecture Notes](http://pages.cs.wisc.edu/~jerryzhu/cs769/nb.pdf); <br> [Mitchell 2nd ed. Ch. 3.1-3.2](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf); <br> [Daum&#233; III, Ch. 9](http://ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf) |   **HW2 Due** <br> **HW3 Out**  |
|   | Feb 27 (we) | Naive Bayes (continued); <br> [Logistic Regression](https://gkunapuli.github.io/files/cs6375/10-LogisticRegression.pdf)  |  [Mitchell 2nd ed. Ch. 3.3-3.5](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf); <br> Bishop, 8.4.1, 9.2, 9.3, 9.4; <br> [Andrew Ng's Lecture Notes, Pt II](http://cs229.stanford.edu/notes/cs229-notes1.pdf); <br> [Kilian Weinberger's Lecture Notes](http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote06.html)  |    |
| 8 | Mar 4 (mo) |  Logistic Regression (continued) |  |**Mid-Term Projects Begin**  |
|   | Mar 6 (we) |    [Ensemble Methods: Bagging](https://gkunapuli.github.io/files/cs6375/11-Bagging.pdf)  | Bishop, Ch. 14; <br> [Hastie et al., Ch. 7.1-7.6, 8.7](https://web.stanford.edu/~hastie/ElemStatLearn/); <br> [Visualization of the Bias-Variance Tradeoff](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)|   |
| 9 | Mar 11 (mo) |  [Ensemble Methods: Boosting](https://gkunapuli.github.io/files/cs6375/12-Boosting.pdf)  | [Hastie et al., Ch. 15](https://web.stanford.edu/~hastie/ElemStatLearn/); <br> [Freund and Schapire, 1999](http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf)  |  **HW3 Due**  |
|   | Mar 13 (we) |   [Ensemble Methods: Gradient Boosting](https://gkunapuli.github.io/files/cs6375/13-GradientBoosting.pdf) | [Friedman, 99](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf); <br> [Mason et al., 99](http://maths.dur.ac.uk/~dma6kp/pdf/face_recognition/Boosting/Mason99AnyboostLong.pdf); <br> [Visualizing Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html); <br> [Tong He's Presentation on XGBoost](https://www.youtube.com/watch?v=ufHo8vbk6g4) |      |
|10 | Mar 18 (mo) | **Spring Break**  <br> _No class_  |    |    |
|   | Mar 20 (we) | **Spring Break**  <br> _No class_ |    |    |
|11 | Mar 25 (mo) |  [Clustering](https://gkunapuli.github.io/files/cs6375/15-Clustering.pdf)   | [Tan et al., Ch. 8](https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf)  |   |
|   | Mar 27 (we) |  Clustering (continued) <br> [Principal Components Analysis](https://gkunapuli.github.io/files/cs6375/14-PrincipalComponentAnalysis.pdf) |  [Andrew Ng's Lecture Notes, Pt II](http://cs229.stanford.edu/notes/cs229-notes10.pdf)  |   **Mid-Term Projects Due** <br> **HW4 Out** |
|12 | Apr 1 (mo) |  Principal Component Analysis (continued)  |   |    |
|   | Apr 3 (we) | [Neural Networks](https://gkunapuli.github.io/files/cs6375/16-NeuralNetworks.pdf)  |   Goodfellow et al., Ch. 6  | **Final Project Teams, Proposal Due**    |
|13 | Apr 8 (mo) | Neural Networks (continued) | |    |
|   | Apr 10 (we) | [Convolutional Neural Networks](https://gkunapuli.github.io/files/cs6375/17-CNNs.pdf)  | Goodfellow et al., Ch. 9  | **HW4 Due**    |
|14 | Apr 15 (mo) | [Recurrent Neural Networks](https://gkunapuli.github.io/files/cs6375/18-RNNs.pdf) | Goodfellow et al., Ch. 10 <br> Kishan Athrey's [Tutorial on Keras](https://www.crcv.ucf.edu/wp-content/uploads/2019/03/CAP6412_Spring2018_KerasTutorial.pdf)  |    |
|   | Apr 17 (we) |  [Other Machine Learning Areas](https://gkunapuli.github.io/files/cs6375/20-OtherMLAreas.pdf)  |    |    |
|15 | Apr 22 (mo) | **Project Presentations** <br> _Attendance is mandatory_   | [Presentations](https://gkunapuli.github.io/files/cs6375/Presentations-Apr22.pdf)   |    |
|   | Apr 24 (we) | **Project Presentations** <br> _Attendance is mandatory_   |    |    |
|16 | Apr 29 (mo) | **Project Presentations** <br> _Attendance is mandatory_   |    |    |
|   | May 1 (we) |  **Project Presentations** <br> _Attendance is mandatory_ <br> _Final classroom sesssion_ |    |   |
|xx | May 6 (mo) | **Final Project Reports Due**  | | |


The topic schedule is subject to change at the instructor's discretion. Please check this page regularly for lecture slides, additional references and reading materials.


Grading
----
The grading rubric is subject to change at the instructor's discretion. Please check this page at the start of the semster.
* 50%, Homework (4, each 12.5%)
* 20%, Mid-term Project
* 30%, Final Project


Course Policies
=====

Attendance Policy
----
Classroom attendance for all lectures is mandatory. Prolonged absence from the lectures may lead to substantial grade penalties:
* two consecutive absences, no penalty;
* 3 consecutive absences: 1 letter grade drop;
* 4 consecutive absences, F grade.

Absence due to emergency or extenuating circumstances can be excused, but [proof may be required](http://cs.utdallas.edu/education/undergraduate/attendance-policy/).

Homework Policy
----
Homework assignments are **due at the start of class on the due date** without exceptions, unless permission was obtained from the instructor **in advance**. Homework and assignment deadlines will  not be extended except under extreme university-wide circumstances such as weather emergencies.

All homeworks, programming projects, take-home exams (if any) **are to be written up and completed individually**. You **may discuss, collaborate, brainstorm and strategize** ideas, concepts and problems with other students. However, all written solutions and coded programs **must be your own**. Copying another student's work or allowing other students to copy your work is academically dishonest.

Academic Integrity
----
All students are responsible for adhering to UT Dallas Community Standards and Conduct, particularly regarding [Academic Integrity](https://www.utdallas.edu/conduct/integrity/) and [Academic Dishonesty](https://www.utdallas.edu/conduct/dishonesty/).  Any academic dishonesty, including, but not restricted to [plagiarism](https://www.utdallas.edu/conduct/dishonesty/#plagiarism) (including from internet sources), [collusion](https://www.utdallas.edu/conduct/dishonesty/#collusion), [cheating](https://www.utdallas.edu/conduct/dishonesty/#cheating), [fabrication](https://www.utdallas.edu/conduct/dishonesty/#fabrication), will result in a zero score on the assignment/project/exam and possible disciplinary action.

Students with Disabilities
----
UT Dallas is committed to equal access in all endeavors for students with disabilities. The [Office of Student Accessability (OSA)](https://www.utdallas.edu/studentaccess/) provides academic accommodations for eligible students with a documented disability. Accommodations for each student are determined by OSA on an individual basis, with input from qualified professionals. Accommodations are intended to level the playing field for students with disabilities, while maintaining the academic integrity and standards set by the University. If you think you qualify for an academic accommodation, please visit OSA to determine eligibility. 

If you have already received academic accommodation, please contact me by e-mail to schedule an appointment **before classes start**, if possible.








